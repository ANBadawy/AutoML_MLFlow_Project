{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 9898134,
          "sourceType": "datasetVersion",
          "datasetId": 6080015
        },
        {
          "sourceId": 9924184,
          "sourceType": "datasetVersion",
          "datasetId": 6099613
        }
      ],
      "dockerImageVersionId": 30787,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "name": "MLFLow_Mistral_Model_Runs"
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "!pip install torch==2.3.0+cu121 torchvision==0.18.0+cu121 torchaudio==2.3.0+cu121 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T14:30:11.298467Z",
          "iopub.execute_input": "2024-11-16T14:30:11.298895Z",
          "iopub.status.idle": "2024-11-16T14:33:49.303883Z",
          "shell.execute_reply.started": "2024-11-16T14:30:11.298848Z",
          "shell.execute_reply": "2024-11-16T14:33:49.302688Z"
        },
        "_kg_hide-output": true,
        "id": "GRpUDUPs4PGi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install mlflow pyngrok"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T14:33:49.305933Z",
          "iopub.execute_input": "2024-11-16T14:33:49.306265Z",
          "iopub.status.idle": "2024-11-16T14:34:08.827484Z",
          "shell.execute_reply.started": "2024-11-16T14:33:49.306231Z",
          "shell.execute_reply": "2024-11-16T14:34:08.826128Z"
        },
        "id": "psntFcGb4PGi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "\n",
        "# Set up MLflow experiment (optional, if not already set)\n",
        "mlflow.set_tracking_uri(\"file:///kaggle/working/mlruns\")  # Saves runs in /kaggle/working/mlruns\n",
        "mlflow.set_experiment(\"Classification_Finetuining_Mistral_Model_Experiment\")\n",
        "mlflow.start_run(run_name=\"Mistral\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T14:34:08.829075Z",
          "iopub.execute_input": "2024-11-16T14:34:08.8295Z",
          "iopub.status.idle": "2024-11-16T14:34:10.976409Z",
          "shell.execute_reply.started": "2024-11-16T14:34:08.829454Z",
          "shell.execute_reply": "2024-11-16T14:34:10.97552Z"
        },
        "id": "zLCvUH574PGi",
        "outputId": "cde7a862-62b1-488d-9d4c-918b332f373a"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2024/11/16 14:34:10 INFO mlflow.tracking.fluent: Experiment with name 'Classification_Finetuining_Mistral_Model_Experiment' does not exist. Creating a new experiment.\n",
          "output_type": "stream"
        },
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<ActiveRun: >"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"2ovuOjLYtKu4y3ljjztiABpxl41_4Kumw4mXHcWeDTWGwUk3L\")\n",
        "\n",
        "# Start MLflow UI\n",
        "get_ipython().system_raw(\"mlflow ui --port 5000 &\")\n",
        "# Expose the MLflow UI on port 5000# Expose the MLflow UI on port 5000\n",
        "public_url = ngrok.connect(\"5000\", \"http\")\n",
        "print(f\"MLflow UI accessible at: {public_url}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T14:34:10.984645Z",
          "iopub.execute_input": "2024-11-16T14:34:10.984999Z",
          "iopub.status.idle": "2024-11-16T14:34:12.029608Z",
          "shell.execute_reply.started": "2024-11-16T14:34:10.984968Z",
          "shell.execute_reply": "2024-11-16T14:34:12.028694Z"
        },
        "id": "MZs1KPIE4PGj",
        "outputId": "285e4ed2-946f-41b3-9353-902376f59e5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "MLflow UI accessible at: NgrokTunnel: \"https://c64c-34-23-62-139.ngrok-free.app\" -> \"http://localhost:5000\"\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "model_id=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_id, #\"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
        "    max_seq_length # Add LoRA adapters so we only need to update 1 to 10% of all parameters!\n",
        "= max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T14:34:12.031194Z",
          "iopub.execute_input": "2024-11-16T14:34:12.03156Z",
          "iopub.status.idle": "2024-11-16T14:35:00.298216Z",
          "shell.execute_reply.started": "2024-11-16T14:34:12.031518Z",
          "shell.execute_reply": "2024-11-16T14:35:00.29707Z"
        },
        "id": "MDWt2PZb4PGk",
        "outputId": "d57f5c37-807d-4f28-f1ad-7a083a8e1c78",
        "colab": {
          "referenced_widgets": [
            "ac9f991236b24c34a274fb11881fbbbf",
            "cb6cdebd43724aa4a93e90afb9d76342",
            "4fa21de726e74e119a300d976a52fb45",
            "a391b8e66dd047beb4f0d77236527e52",
            "0bf0b7045086480d9d220d697146b6d7",
            "4dc9f4c84bd04595b179cd19cc292bc8",
            "503a6c0190d640e580b75755c4e6b127",
            "f21f0a7113cd4739bc644b244d7d539d",
            "92662066b9fc4f8fb4a05a4716a35f19",
            "dac30d5ab6cb4646a9ab753f058021dd",
            "308d6a240fcf436998376e5cecb5d6c5"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[2024-11-16 14:34:14 +0000] [216] [INFO] Starting gunicorn 23.0.0\n[2024-11-16 14:34:14 +0000] [216] [INFO] Listening at: http://127.0.0.1:5000 (216)\n[2024-11-16 14:34:14 +0000] [216] [INFO] Using worker: sync\n[2024-11-16 14:34:14 +0000] [217] [INFO] Booting worker with pid: 217\n[2024-11-16 14:34:14 +0000] [218] [INFO] Booting worker with pid: 218\n[2024-11-16 14:34:14 +0000] [219] [INFO] Booting worker with pid: 219\n[2024-11-16 14:34:14 +0000] [220] [INFO] Booting worker with pid: 220\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "==((====))==  Unsloth 2024.11.7: Fast Mistral patching. Transformers = 4.46.2.\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/4.13G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4dc9f4c84bd04595b179cd19cc292bc8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/155 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "503a6c0190d640e580b75755c4e6b127"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/2.13k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f21f0a7113cd4739bc644b244d7d539d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92662066b9fc4f8fb4a05a4716a35f19"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dac30d5ab6cb4646a9ab753f058021dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "308d6a240fcf436998376e5cecb5d6c5"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ],
      "metadata": {
        "id": "3pdONVmj4PGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary of parameters to log\n",
        "model_params = {\n",
        "    \"Mistral_model_name\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    \"Mistral_max_seq_length\": max_seq_length,\n",
        "    \"Mistral_dtype\": dtype,\n",
        "    \"Mistral_load_in_4bit\": load_in_4bit,\n",
        "}\n",
        "\n",
        "# Log all parameters at once\n",
        "mlflow.log_params(model_params)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T14:35:00.299721Z",
          "iopub.execute_input": "2024-11-16T14:35:00.300081Z",
          "iopub.status.idle": "2024-11-16T14:35:00.308099Z",
          "shell.execute_reply.started": "2024-11-16T14:35:00.300047Z",
          "shell.execute_reply": "2024-11-16T14:35:00.307292Z"
        },
        "id": "K9b8Uocn4PGl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",\"lm_head\"],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0.1, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = True, # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T14:35:00.309397Z",
          "iopub.execute_input": "2024-11-16T14:35:00.309785Z",
          "iopub.status.idle": "2024-11-16T14:35:04.902264Z",
          "shell.execute_reply.started": "2024-11-16T14:35:00.309744Z",
          "shell.execute_reply": "2024-11-16T14:35:04.901225Z"
        },
        "id": "ZlEdYapd4PGl",
        "outputId": "9da21606-fc09-41fe-c273-968674ec40b1"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.1.\nUnsloth will patch all other layers, except LoRA matrices, causing a performance hit.\nUnsloth 2024.11.7 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Unsloth: Training lm_head in mixed precision to save VRAM\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.padding_side = 'right'\n",
        "tokenizer.add_eos_token = True\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T14:35:04.903649Z",
          "iopub.execute_input": "2024-11-16T14:35:04.903995Z",
          "iopub.status.idle": "2024-11-16T14:35:04.908753Z",
          "shell.execute_reply.started": "2024-11-16T14:35:04.90396Z",
          "shell.execute_reply": "2024-11-16T14:35:04.907828Z"
        },
        "id": "SuwOMKvo4PGm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model_params = {\n",
        "    \"LoRA_r\": 64,\n",
        "    \"LoRA_target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\"],\n",
        "    \"LoRA_lora_alpha\": 16,\n",
        "    \"LoRA_lora_dropout\": 0.1,\n",
        "    \"LoRA_bias\": \"none\",\n",
        "    \"LoRA_use_gradient_checkpointing\": True,\n",
        "    \"LoRA_random_state\": 3407,\n",
        "    \"LoRA_use_rslora\": False,\n",
        "    \"LoRA_loftq_config\": None,\n",
        "}\n",
        "\n",
        "# Log model parameters\n",
        "mlflow.log_params(model_params)\n",
        "\n",
        "# Define tokenizer settings for logging\n",
        "tokenizer_settings = {\n",
        "    \"tokenizer_padding_side\": \"right\",\n",
        "    \"tokenizer_add_eos_token\": True,\n",
        "    \"tokenizerpad_token\": tokenizer.eos_token,\n",
        "}\n",
        "\n",
        "# Log tokenizer settings\n",
        "mlflow.log_params(tokenizer_settings)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T14:35:04.913342Z",
          "iopub.execute_input": "2024-11-16T14:35:04.91362Z",
          "iopub.status.idle": "2024-11-16T14:35:04.925083Z",
          "shell.execute_reply.started": "2024-11-16T14:35:04.91359Z",
          "shell.execute_reply": "2024-11-16T14:35:04.924051Z"
        },
        "id": "3tZVrbwG4PGm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "dHZjC0YZ4PGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import mlflow\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset('csv', data_files='/kaggle/input/train-dataset/Classification_train.csv', split='train')\n",
        "\n",
        "# Log dataset parameters with MLflow\n",
        "mlflow.log_param(\"dataset_name\", \"Classification_train.csv\")\n",
        "mlflow.log_param(\"num_rows\", dataset.num_rows)\n",
        "mlflow.log_param(\"num_columns\", len(dataset.column_names))\n",
        "mlflow.log_param(\"column_names\", dataset.column_names)\n",
        "\n",
        "# Convert to pandas DataFrame for manipulation\n",
        "classification_train = dataset.to_pandas()\n",
        "\n",
        "# Take a 10% sample for the test dataset\n",
        "classification_test = classification_train.sample(frac=0.1, random_state=42)\n",
        "\n",
        "# Drop the sampled rows from the original dataset to get the remaining 90%\n",
        "classification_train = classification_train.drop(classification_test.index)\n",
        "\n",
        "# Save the test and updated train datasets\n",
        "classification_test.to_csv('classification_test.csv', index=False)\n",
        "classification_train.to_csv('classification_train.csv', index=False)\n",
        "\n",
        "print(\"10% sample saved as classification_test.csv and removed from classification_train.csv\")\n",
        "\n",
        "# Log a sample of the dataset as an artifact\n",
        "sample_df = dataset.to_pandas().sample(5)  # Taking a small sample for logging\n",
        "sample_df.to_csv(\"dataset_sample.csv\", index=False)\n",
        "mlflow.log_artifact(\"dataset_sample.csv\", artifact_path=\"dataset_sample\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T14:35:04.926377Z",
          "iopub.execute_input": "2024-11-16T14:35:04.926772Z",
          "iopub.status.idle": "2024-11-16T14:35:05.309918Z",
          "shell.execute_reply.started": "2024-11-16T14:35:04.92673Z",
          "shell.execute_reply": "2024-11-16T14:35:05.308965Z"
        },
        "id": "I6DmwwoK4PGm",
        "outputId": "3129b405-8bc6-410f-d33e-9be38d60724f",
        "colab": {
          "referenced_widgets": [
            "e066214122a0439ea2f5bf2ac30b6d8b",
            "af513ca5e346416ca5a948a61dc7541f"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af513ca5e346416ca5a948a61dc7541f"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "10% sample saved as classification_test.csv and removed from classification_train.csv\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "classification_train = pd.read_csv('/kaggle/input/train-dataset/Classification_train.csv')\n",
        "\n",
        "# Take a 10% sample for the test dataset\n",
        "classification_test = classification_train.sample(frac=0.1, random_state=42)\n",
        "\n",
        "# Drop the sampled rows from the original dataset to get the remaining 90%\n",
        "classification_train = classification_train.drop(classification_test.index)\n",
        "\n",
        "# Save the test and updated train datasets\n",
        "classification_test.to_csv('classification_test.csv', index=False)\n",
        "classification_train.to_csv('classification_train.csv', index=False)\n",
        "\n",
        "print(\"10% sample saved as classification_test.csv and removed from classification_train.csv\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T14:35:05.311421Z",
          "iopub.execute_input": "2024-11-16T14:35:05.311786Z",
          "iopub.status.idle": "2024-11-16T14:35:05.407645Z",
          "shell.execute_reply.started": "2024-11-16T14:35:05.311749Z",
          "shell.execute_reply": "2024-11-16T14:35:05.406684Z"
        },
        "id": "iClwRrYr4PGn",
        "outputId": "5decfd24-45be-4e25-c725-877e31f25958"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "10% sample saved as classification_test.csv and removed from classification_train.csv\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset('csv', data_files='/kaggle/working/classification_train.csv', split='train')\n",
        "dataset"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T14:35:05.408991Z",
          "iopub.execute_input": "2024-11-16T14:35:05.409324Z",
          "iopub.status.idle": "2024-11-16T14:35:05.505114Z",
          "shell.execute_reply.started": "2024-11-16T14:35:05.409289Z",
          "shell.execute_reply": "2024-11-16T14:35:05.504202Z"
        },
        "id": "aKONNt4V4PGn",
        "outputId": "d4e3b554-a5b4-4132-c22c-e638d26711e4",
        "colab": {
          "referenced_widgets": [
            "53871d3ea0634b1b975ec01a605a377c",
            "e4887ea58a824e3c819fbafd84d5a6cf"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4887ea58a824e3c819fbafd84d5a6cf"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['dataset_name', 'series_description', 'algorithm', 'hyperparameters'],\n    num_rows: 727\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_valid_split = dataset.train_test_split(test_size=0.1)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T14:35:05.506353Z",
          "iopub.execute_input": "2024-11-16T14:35:05.507169Z",
          "iopub.status.idle": "2024-11-16T14:35:05.524566Z",
          "shell.execute_reply.started": "2024-11-16T14:35:05.507134Z",
          "shell.execute_reply": "2024-11-16T14:35:05.523687Z"
        },
        "id": "DVlr3GK44PGo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_valid_split['train']\n",
        "valid_dataset = train_valid_split['test']"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T14:35:05.525894Z",
          "iopub.execute_input": "2024-11-16T14:35:05.526195Z",
          "iopub.status.idle": "2024-11-16T14:35:05.530609Z",
          "shell.execute_reply.started": "2024-11-16T14:35:05.526165Z",
          "shell.execute_reply": "2024-11-16T14:35:05.529612Z"
        },
        "id": "4wGshMZv4PGo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "train_prompt = \"\"\"Below is a description for a time series data. Write a response that gives the name of the best fitting machine learning algorithm in one word without explanation.\n",
        "The best algorithm name should be one of this search space algorithms: AdaboostRegressor, ElasticNetRegressor,  ExtraTreesRegressor,  LassoRegressor,  LightgbmRegressor, SVR, GaussianProcessRegressor, RandomForestRegressor or  XGBoostRegressor.\n",
        "\n",
        "### DESCRIPTION:\n",
        "{}\n",
        "\n",
        "### RESPONSE:\n",
        "{}\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    inputs       = examples[\"series_description\"]\n",
        "    outputs      = examples[\"algorithm\"]\n",
        "    texts = []\n",
        "    for input, output in zip( inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = train_prompt.format( input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "\n",
        "def formatting_test_prompts_func(examples):\n",
        "    inputs = examples[\"series_description\"]\n",
        "    texts = []\n",
        "\n",
        "    for input in inputs:\n",
        "        text = test_prompt.format(input)\n",
        "        texts.append(text)\n",
        "\n",
        "    return { \"text\": texts }"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T14:35:05.531816Z",
          "iopub.execute_input": "2024-11-16T14:35:05.532145Z",
          "iopub.status.idle": "2024-11-16T14:35:05.540965Z",
          "shell.execute_reply.started": "2024-11-16T14:35:05.532114Z",
          "shell.execute_reply": "2024-11-16T14:35:05.540179Z"
        },
        "id": "T8vxq1Be4PGo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.map(formatting_prompts_func, batched = True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T14:35:05.542135Z",
          "iopub.execute_input": "2024-11-16T14:35:05.542438Z",
          "iopub.status.idle": "2024-11-16T14:35:05.601205Z",
          "shell.execute_reply.started": "2024-11-16T14:35:05.542403Z",
          "shell.execute_reply": "2024-11-16T14:35:05.60033Z"
        },
        "id": "JDq3Mz4N4PGo",
        "outputId": "d2dabe1b-435b-468f-af3f-b1ff1b463a94",
        "colab": {
          "referenced_widgets": [
            "76625a65f5cc463ea89902f6c9158d04",
            "4c088fff8c8e48c2a4bce0ecfc47234f"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/654 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c088fff8c8e48c2a4bce0ecfc47234f"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T14:35:05.602308Z",
          "iopub.execute_input": "2024-11-16T14:35:05.6026Z",
          "iopub.status.idle": "2024-11-16T14:35:05.60822Z",
          "shell.execute_reply.started": "2024-11-16T14:35:05.602569Z",
          "shell.execute_reply": "2024-11-16T14:35:05.607389Z"
        },
        "id": "xaoOlvNN4PGo",
        "outputId": "c8c11804-e1f8-48d6-9cfa-6ef180d4ac6a"
      },
      "outputs": [
        {
          "execution_count": 18,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['dataset_name', 'series_description', 'algorithm', 'hyperparameters', 'text'],\n    num_rows: 654\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "valid_dataset = valid_dataset.map(formatting_prompts_func, batched = True)\n",
        "valid_dataset"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T14:35:05.609522Z",
          "iopub.execute_input": "2024-11-16T14:35:05.610232Z",
          "iopub.status.idle": "2024-11-16T14:35:05.646494Z",
          "shell.execute_reply.started": "2024-11-16T14:35:05.61019Z",
          "shell.execute_reply": "2024-11-16T14:35:05.645657Z"
        },
        "id": "q10qdweL4PGo",
        "outputId": "08ee89da-2f38-465e-f730-038b466665e0",
        "colab": {
          "referenced_widgets": [
            "6b6b2f3542c14f439c704be8ab9524bf",
            "08db861912f64710b0a47ec218cb4cfa"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/73 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "08db861912f64710b0a47ec218cb4cfa"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 19,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['dataset_name', 'series_description', 'algorithm', 'hyperparameters', 'text'],\n    num_rows: 73\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "training_arguments= TrainingArguments(\n",
        "        num_train_epochs=1,\n",
        "        per_device_train_batch_size = 4,\n",
        "        gradient_accumulation_steps = 32//4,\n",
        "        gradient_checkpointing=True,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = -1, # Set num_train_epochs = 1 for full training runs\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"./mistral_outputs\",\n",
        "        evaluation_strategy=\"steps\", #epoch\n",
        "        save_strategy=\"epoch\",\n",
        "    )"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T14:35:05.647612Z",
          "iopub.execute_input": "2024-11-16T14:35:05.647946Z",
          "iopub.status.idle": "2024-11-16T14:35:05.696555Z",
          "shell.execute_reply.started": "2024-11-16T14:35:05.647914Z",
          "shell.execute_reply": "2024-11-16T14:35:05.695642Z"
        },
        "id": "0f3Q8sOp4PGp",
        "outputId": "361c74e9-b0c7-45db-e1ea-f1313f557da5"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import  DataCollatorForCompletionOnlyLM\n",
        "\n",
        "instruction_template=\"DESCRIPTION:\"\n",
        "response_template = \"RESPONSE:\"\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = training_arguments,\n",
        "    # data_collator =  DataCollatorForCompletionOnlyLM(instruction_template=instruction_template,\n",
        "    #                                                  response_template=response_template,\n",
        "    #                                                  tokenizer=tokenizer,mlm=False),\n",
        "\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T14:35:05.697828Z",
          "iopub.execute_input": "2024-11-16T14:35:05.698132Z",
          "iopub.status.idle": "2024-11-16T14:35:07.876997Z",
          "shell.execute_reply.started": "2024-11-16T14:35:05.698101Z",
          "shell.execute_reply": "2024-11-16T14:35:07.876078Z"
        },
        "id": "9UvLPIoP4PGp",
        "outputId": "6d4988ad-0120-4c9f-825b-2d15aa1b918e",
        "colab": {
          "referenced_widgets": [
            "e08e7889801e4a0eb9191f35876f6283",
            "881890d2e85c4347b43e8314915f9346",
            "afa933c1f78c494d9ca1f039e32216ae",
            "790656c0baed441781b6c85855d1a180"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map (num_proc=2):   0%|          | 0/654 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "afa933c1f78c494d9ca1f039e32216ae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map (num_proc=2):   0%|          | 0/73 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "790656c0baed441781b6c85855d1a180"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T14:35:07.878469Z",
          "iopub.execute_input": "2024-11-16T14:35:07.87888Z",
          "iopub.status.idle": "2024-11-16T14:35:07.885865Z",
          "shell.execute_reply.started": "2024-11-16T14:35:07.878835Z",
          "shell.execute_reply": "2024-11-16T14:35:07.884832Z"
        },
        "id": "yNepVx-O4PGq",
        "outputId": "b81aec9d-a731-4b15-a5db-52148a535f8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "GPU = Tesla T4. Max memory = 14.741 GB.\n5.715 GB of memory reserved.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start= time.time()\n",
        "trainer_stats = trainer.train()\n",
        "print((time.time()-start)/60)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T14:35:07.887076Z",
          "iopub.execute_input": "2024-11-16T14:35:07.887361Z",
          "iopub.status.idle": "2024-11-16T15:26:35.854389Z",
          "shell.execute_reply.started": "2024-11-16T14:35:07.887331Z",
          "shell.execute_reply": "2024-11-16T15:26:35.852321Z"
        },
        "id": "XMraUVbv4PGq",
        "outputId": "68ae8caf-587a-4679-89aa-c2def518567f",
        "colab": {
          "referenced_widgets": [
            "a4cd7087f926444a89c07d1fd3645426",
            "22e122055a774f99aa81895765d98dd4"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 654 | Num Epochs = 1\nO^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 8\n\\        /    Total batch size = 32 | Total steps = 20\n \"-____-\"     Number of trainable parameters = 298,844,160\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "  Â·Â·Â·Â·Â·Â·Â·Â·\n"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113666055555567, max=1.0â€¦",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22e122055a774f99aa81895765d98dd4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.18.3"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20241116_144504-cnsrnm0i</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/wolfteampro930-nile-university/huggingface/runs/cnsrnm0i' target=\"_blank\">./mistral_outputs</a></strong> to <a href='https://wandb.ai/wolfteampro930-nile-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/wolfteampro930-nile-university/huggingface' target=\"_blank\">https://wandb.ai/wolfteampro930-nile-university/huggingface</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/wolfteampro930-nile-university/huggingface/runs/cnsrnm0i' target=\"_blank\">https://wandb.ai/wolfteampro930-nile-university/huggingface/runs/cnsrnm0i</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [20/20 40:12, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.235300</td>\n      <td>2.229566</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.248900</td>\n      <td>1.811847</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.816900</td>\n      <td>1.269612</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.272800</td>\n      <td>0.876614</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.870700</td>\n      <td>0.624159</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.641400</td>\n      <td>0.468591</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.469700</td>\n      <td>0.354218</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.352700</td>\n      <td>0.294866</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.298800</td>\n      <td>0.263922</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.283200</td>\n      <td>0.246096</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.238900</td>\n      <td>0.233613</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.211700</td>\n      <td>0.228128</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.253200</td>\n      <td>0.224755</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.245600</td>\n      <td>0.220940</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.214900</td>\n      <td>0.217308</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.202700</td>\n      <td>0.215053</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.235600</td>\n      <td>0.212961</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.211300</td>\n      <td>0.211717</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.206200</td>\n      <td>0.210815</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.210100</td>\n      <td>0.210249</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "51.465876722335814\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T15:38:44.341612Z",
          "iopub.execute_input": "2024-11-16T15:38:44.342539Z",
          "iopub.status.idle": "2024-11-16T15:38:44.353029Z",
          "shell.execute_reply.started": "2024-11-16T15:38:44.342497Z",
          "shell.execute_reply": "2024-11-16T15:38:44.352061Z"
        },
        "id": "AUXV_WzA4PGq",
        "outputId": "67123cb8-e079-4358-e2f3-bc19c783aeeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "3084.6466 seconds used for training.\n51.41 minutes used for training.\nPeak reserved memory = 10.781 GB.\nPeak reserved memory for training = 5.066 GB.\nPeak reserved memory % of max memory = 73.136 %.\nPeak reserved memory for training % of max memory = 34.367 %.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "mlflow.end_run()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T15:38:45.347724Z",
          "iopub.execute_input": "2024-11-16T15:38:45.348123Z",
          "iopub.status.idle": "2024-11-16T15:38:45.353638Z",
          "shell.execute_reply.started": "2024-11-16T15:38:45.348084Z",
          "shell.execute_reply": "2024-11-16T15:38:45.352692Z"
        },
        "id": "cV5LQ8llA9Zw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r mlruns.zip /kaggle/working/mlruns"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T15:38:51.956683Z",
          "iopub.execute_input": "2024-11-16T15:38:51.957469Z",
          "iopub.status.idle": "2024-11-16T15:38:53.019595Z",
          "shell.execute_reply.started": "2024-11-16T15:38:51.95743Z",
          "shell.execute_reply": "2024-11-16T15:38:53.018453Z"
        },
        "id": "bBvCyUQmA9Zw",
        "outputId": "464a6978-3e84-4f56-d25f-a4319246ed4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "updating: kaggle/working/mlruns/ (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/ (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/ (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/metrics/ (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/metrics/eval_steps_per_second (deflated 66%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/metrics/train_samples_per_second (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/metrics/loss (deflated 58%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/metrics/train_loss (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/metrics/learning_rate (deflated 67%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/metrics/total_flos (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/metrics/eval_loss (deflated 55%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/metrics/train_runtime (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/metrics/train_steps_per_second (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/metrics/eval_samples_per_second (deflated 63%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/metrics/eval_runtime (deflated 58%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/metrics/epoch (deflated 77%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/metrics/grad_norm (deflated 53%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/tags/ (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/tags/mlflow.source.type (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/tags/mlflow.user (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/tags/mlflow.runName (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/tags/mlflow.source.name (deflated 3%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/artifacts/ (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/artifacts/dataset_sample/ (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/artifacts/dataset_sample/dataset_sample.csv (deflated 85%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/meta.yaml (deflated 44%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/ (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/dataloader_pin_memory (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/LoRA_r (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/Mistral_max_seq_length (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/decoder_start_token_id (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/tokenizerpad_token (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/max_position_embeddings (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/max_length (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/sep_token_id (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/remove_unused_columns (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/eval_do_concat_batches (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/LoRA_lora_dropout (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/torchscript (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/eval_on_start (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/log_level_replica (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/fsdp_config (deflated 33%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/do_predict (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/warmup_steps (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/group_by_length (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/label_names (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/lr_scheduler_type (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/sliding_window (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/exponential_decay_length_penalty (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/begin_suppress_tokens (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/use_liger_kernel (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/do_train (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/hub_token (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/LoRA_use_gradient_checkpointing (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/ray_scope (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/num_beam_groups (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/save_safetensors (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/prediction_loss_only (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/prefix (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/num_columns (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/per_device_eval_batch_size (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/per_device_train_batch_size (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/restore_callback_states_from_checkpoint (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/accelerator_config (deflated 33%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/intermediate_size (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/dataloader_drop_last (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/max_grad_norm (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/tie_word_embeddings (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/remove_invalid_values (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/use_ipex (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/use_cache (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/diversity_penalty (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/use_legacy_prediction_loop (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/fp16_backend (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/LoRA_loftq_config (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/hub_always_push (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/tf_legacy_loss (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/dataloader_num_workers (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/warmup_ratio (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/include_inputs_for_metrics (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/metric_for_best_model (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/_name_or_path (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/log_on_each_node (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/rope_scaling (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/tokenizer_padding_side (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/no_cuda (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/hub_strategy (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/eos_token_id (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/adam_epsilon (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/id2label (deflated 21%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/eval_steps (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/output_scores (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/typical_p (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/fp16 (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/tpu_metrics_debug (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/num_rows (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/tokenizer_add_eos_token (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/half_precision_backend (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/use_mps_device (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/torch_empty_cache_steps (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/run_name (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/num_return_sequences (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/hidden_size (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/save_steps (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/length_penalty (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/push_to_hub_organization (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/num_train_epochs (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/suppress_tokens (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/return_dict (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/LoRA_target_modules (deflated 48%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/push_to_hub (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/pad_token_id (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/logging_strategy (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/torch_dtype (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/lr_scheduler_kwargs (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/fp16_full_eval (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/learning_rate (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/do_eval (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/logging_nan_inf_filter (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/use_cpu (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/optim (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/hidden_act (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/pruned_heads (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/save_total_limit (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/ddp_bucket_cap_mb (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/torchdynamo (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/label2id (deflated 18%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/report_to (deflated 3%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/column_names (deflated 6%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/return_dict_in_generate (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/torch_compile (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/bf16_full_eval (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/add_cross_attention (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/logging_steps (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/adam_beta1 (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/top_p (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/Mistral_load_in_4bit (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/include_for_metrics (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/bf16 (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/temperature (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/is_decoder (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/save_on_each_node (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/dataset_name (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/past_index (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/bos_token_id (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/greater_is_better (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/data_seed (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/ddp_timeout (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/hub_private_repo (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/is_encoder_decoder (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/torch_compile_backend (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/push_to_hub_token (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/tokenizer_class (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/initializer_range (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/jit_mode_eval (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/load_best_model_at_end (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/LoRA_bias (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/per_gpu_train_batch_size (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/eval_accumulation_steps (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/label_smoothing_factor (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/num_attention_heads (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/tpu_num_cores (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/average_tokens_across_devices (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/Mistral_model_name (deflated 12%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/full_determinism (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/min_length (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/fsdp (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/debug (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/num_hidden_layers (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/max_steps (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/finetuning_task (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/do_sample (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/architectures (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/deepspeed (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/adafactor (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/ddp_find_unused_parameters (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/dispatch_batches (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/output_attentions (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/LoRA_use_rslora (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/unsloth_version (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/neftune_noise_alpha (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/bad_words_ids (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/per_gpu_eval_batch_size (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/batch_eval_metrics (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/length_column_name (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/tie_encoder_decoder (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/LoRA_random_state (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/Mistral_dtype (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/vocab_size (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/transformers_version (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/auto_find_batch_size (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/save_strategy (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/num_beams (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/fsdp_transformer_layer_cls_to_wrap (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/local_rank (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/dataloader_persistent_workers (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/use_bfloat16 (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/torch_compile_mode (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/save_only_model (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/weight_decay (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/eval_strategy (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/eval_delay (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/output_dir (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/rms_norm_eps (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/num_key_value_heads (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/seed (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/eval_use_gather_object (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/no_repeat_ngram_size (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/mp_parameters (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/output_hidden_states (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/dataloader_prefetch_factor (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/forced_bos_token_id (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/chunk_size_feed_forward (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/hub_model_id (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/rope_theta (deflated 22%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/cross_attention_hidden_size (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/logging_dir (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/_attn_implementation_autoset (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/attention_dropout (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/ignore_data_skip (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/include_tokens_per_second (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/encoder_no_repeat_ngram_size (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/gradient_accumulation_steps (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/forced_eos_token_id (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/fp16_opt_level (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/gradient_checkpointing_kwargs (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/push_to_hub_model_id (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/tf32 (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/LoRA_lora_alpha (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/evaluation_strategy (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/skip_memory_metrics (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/resume_from_checkpoint (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/ddp_backend (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/task_specific_params (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/problem_type (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/disable_tqdm (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/optim_args (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/top_k (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/include_num_input_tokens_seen (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/ddp_broadcast_buffers (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/head_dim (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/overwrite_output_dir (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/model_type (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/split_batches (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/early_stopping (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/quantization_config (deflated 44%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/fsdp_min_num_params (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/adam_beta2 (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/optim_target_modules (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/log_level (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/repetition_penalty (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/logging_first_step (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/68e2d839c3a54258a9c5fd35dbb3b2f0/params/gradient_checkpointing (stored 0%)\nupdating: kaggle/working/mlruns/446966349057200023/meta.yaml (deflated 31%)\nupdating: kaggle/working/mlruns/.trash/ (stored 0%)\nupdating: kaggle/working/mlruns/0/ (stored 0%)\nupdating: kaggle/working/mlruns/0/meta.yaml (deflated 24%)\nupdating: kaggle/working/mlruns/models/ (stored 0%)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "vjzYMIBxA9Zw"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}