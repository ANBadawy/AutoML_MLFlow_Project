{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9907752,"sourceType":"datasetVersion","datasetId":6087213}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n\n# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n!pip install \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n!pip install torch==2.3.0+cu121 torchvision==0.18.0+cu121 torchaudio==2.3.0+cu121 -f https://download.pytorch.org/whl/torch_stable.html","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:14:03.073909Z","iopub.execute_input":"2024-11-17T18:14:03.074206Z","iopub.status.idle":"2024-11-17T18:17:46.249117Z","shell.execute_reply.started":"2024-11-17T18:14:03.074174Z","shell.execute_reply":"2024-11-17T18:17:46.247837Z"},"_kg_hide-output":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"%%capture\n!pip install mlflow pyngrok","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:17:46.251199Z","iopub.execute_input":"2024-11-17T18:17:46.251538Z","iopub.status.idle":"2024-11-17T18:18:06.175923Z","shell.execute_reply.started":"2024-11-17T18:17:46.251493Z","shell.execute_reply":"2024-11-17T18:18:06.174601Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import mlflow\n\n# Set up MLflow experiment (optional, if not already set)\nmlflow.set_tracking_uri(\"file:///kaggle/working/mlruns\")  # Saves runs in /kaggle/working/mlruns\nmlflow.set_experiment(\"Classification_Finetuining_Experiment\")\nmlflow.start_run(run_name=\"Llama-3.2\") # write here your model name\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:18:06.177358Z","iopub.execute_input":"2024-11-17T18:18:06.177783Z","iopub.status.idle":"2024-11-17T18:18:09.112528Z","shell.execute_reply.started":"2024-11-17T18:18:06.177735Z","shell.execute_reply":"2024-11-17T18:18:09.111611Z"}},"outputs":[{"name":"stderr","text":"2024/11/17 18:18:08 INFO mlflow.tracking.fluent: Experiment with name 'Classification_Finetuining_Experiment' does not exist. Creating a new experiment.\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"<ActiveRun: >"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"mlflow.end_run()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:18:09.114449Z","iopub.execute_input":"2024-11-17T18:18:09.114778Z","iopub.status.idle":"2024-11-17T18:18:09.120670Z","shell.execute_reply.started":"2024-11-17T18:18:09.114744Z","shell.execute_reply":"2024-11-17T18:18:09.119842Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from pyngrok import ngrok\nngrok.set_auth_token(\"2orh5SJMd1vIGXN23P3WVlO0Dqx_6N7mnS8ZbXGsgFbwuFtv5\")\n\n# Start MLflow UI\nget_ipython().system_raw(\"mlflow ui --port 5000 &\")\n# Expose the MLflow UI on port 5000# Expose the MLflow UI on port 5000\npublic_url = ngrok.connect(\"5000\", \"http\")\nprint(f\"MLflow UI accessible at: {public_url}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:18:09.122075Z","iopub.execute_input":"2024-11-17T18:18:09.122743Z","iopub.status.idle":"2024-11-17T18:18:10.752401Z","shell.execute_reply.started":"2024-11-17T18:18:09.122698Z","shell.execute_reply":"2024-11-17T18:18:10.751342Z"}},"outputs":[{"name":"stdout","text":"MLflow UI accessible at: NgrokTunnel: \"https://77da-34-168-186-70.ngrok-free.app\" -> \"http://localhost:5000\"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-3B-bnb-4bit\",\n    max_seq_length # Add LoRA adapters so we only need to update 1 to 10% of all parameters!\n= max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:18:10.753885Z","iopub.execute_input":"2024-11-17T18:18:10.754268Z","iopub.status.idle":"2024-11-17T18:18:50.056861Z","shell.execute_reply.started":"2024-11-17T18:18:10.754215Z","shell.execute_reply":"2024-11-17T18:18:50.055933Z"}},"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"[2024-11-17 18:18:13 +0000] [188] [INFO] Starting gunicorn 23.0.0\n[2024-11-17 18:18:13 +0000] [188] [INFO] Listening at: http://127.0.0.1:5000 (188)\n[2024-11-17 18:18:13 +0000] [188] [INFO] Using worker: sync\n[2024-11-17 18:18:13 +0000] [190] [INFO] Booting worker with pid: 190\n[2024-11-17 18:18:13 +0000] [191] [INFO] Booting worker with pid: 191\n[2024-11-17 18:18:13 +0000] [192] [INFO] Booting worker with pid: 192\n[2024-11-17 18:18:13 +0000] [193] [INFO] Booting worker with pid: 193\n","output_type":"stream"},{"name":"stdout","text":"==((====))==  Unsloth 2024.11.7: Fast Llama patching. Transformers = 4.46.2.\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3831b2a513ce45658e99d031f77d204b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/121 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"756ff225f5d24a3aaae756eccff8bce4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f67c425e83442f4a5b96425feb1ed37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4eb2c393a1ea484a94a0aebc9fdf66c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e84c9c9a119640eeb8782eaa9b80a990"}},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"# Add LoRA adapters so we only need to update 1 to 10% of all parameters!\n","metadata":{}},{"cell_type":"code","source":"# Create a dictionary of parameters to log\nmodel_params = {\n    \"Llama_model_name\": \"unsloth/Llama-3.2-3B-bnb-4bit\",\n    \"Llama_max_seq_length\": max_seq_length,\n    \"Llama_dtype\": dtype,\n    \"Llama_load_in_4bit\": load_in_4bit,\n}\n\n# Log all parameters at once\nmlflow.log_params(model_params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:18:50.058337Z","iopub.execute_input":"2024-11-17T18:18:50.058785Z","iopub.status.idle":"2024-11-17T18:18:50.077538Z","shell.execute_reply.started":"2024-11-17T18:18:50.058743Z","shell.execute_reply":"2024-11-17T18:18:50.076668Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",\"lm_head\"],\n    lora_alpha = 16,\n    lora_dropout = 0.1, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = True, # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:18:50.078764Z","iopub.execute_input":"2024-11-17T18:18:50.079054Z","iopub.status.idle":"2024-11-17T18:18:56.193962Z","shell.execute_reply.started":"2024-11-17T18:18:50.079023Z","shell.execute_reply":"2024-11-17T18:18:56.193125Z"}},"outputs":[{"name":"stderr","text":"Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.1.\nUnsloth will patch all other layers, except LoRA matrices, causing a performance hit.\nUnsloth 2024.11.7 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Training lm_head in mixed precision to save VRAM\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"tokenizer.padding_side = 'right'\ntokenizer.add_eos_token = True\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:18:56.195234Z","iopub.execute_input":"2024-11-17T18:18:56.195575Z","iopub.status.idle":"2024-11-17T18:18:56.200066Z","shell.execute_reply.started":"2024-11-17T18:18:56.195541Z","shell.execute_reply":"2024-11-17T18:18:56.199125Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"model_params = {\n    \"LoRA_r\": 64,\n    \"LoRA_target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\"],\n    \"LoRA_lora_alpha\": 16,\n    \"LoRA_lora_dropout\": 0.1,\n    \"LoRA_bias\": \"none\",\n    \"LoRA_use_gradient_checkpointing\": True,\n    \"LoRA_random_state\": 3407,\n    \"LoRA_use_rslora\": False,\n    \"LoRA_loftq_config\": None,\n}\n\n# Log model parameters\nmlflow.log_params(model_params)\n\n# Define tokenizer settings for logging\ntokenizer_settings = {\n    \"tokenizer_padding_side\": \"right\",\n    \"tokenizer_add_eos_token\": True,\n    \"tokenizerpad_token\": tokenizer.eos_token,\n}\n\n# Log tokenizer settings\nmlflow.log_params(tokenizer_settings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:18:56.204080Z","iopub.execute_input":"2024-11-17T18:18:56.204487Z","iopub.status.idle":"2024-11-17T18:18:56.217958Z","shell.execute_reply.started":"2024-11-17T18:18:56.204454Z","shell.execute_reply":"2024-11-17T18:18:56.217106Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Data Preparation","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nclassification_train = pd.read_csv('/kaggle/input/train-dataset/Classification_train.csv')\n\n# Take a 10% sample for the test dataset\nclassification_test = classification_train.sample(frac=0.1, random_state=42)\n\n# Drop the sampled rows from the original dataset to get the remaining 90%\nclassification_train = classification_train.drop(classification_test.index)\n\n# Save the test and updated train datasets\nclassification_test.to_csv('classification_test.csv', index=False)\nclassification_train.to_csv('classification_train.csv', index=False)\n\nprint(\"10% sample saved as classification_test.csv and removed from classification_train.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:18:56.219120Z","iopub.execute_input":"2024-11-17T18:18:56.219433Z","iopub.status.idle":"2024-11-17T18:18:56.365567Z","shell.execute_reply.started":"2024-11-17T18:18:56.219399Z","shell.execute_reply":"2024-11-17T18:18:56.364653Z"}},"outputs":[{"name":"stdout","text":"10% sample saved as classification_test.csv and removed from classification_train.csv\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset('csv', data_files='/kaggle/working/classification_train.csv', split='train')\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:18:56.366801Z","iopub.execute_input":"2024-11-17T18:18:56.367115Z","iopub.status.idle":"2024-11-17T18:18:56.756124Z","shell.execute_reply.started":"2024-11-17T18:18:56.367083Z","shell.execute_reply":"2024-11-17T18:18:56.755280Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6322cbf8e695473fad0eac110bd28580"}},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['dataset_name', 'series_description', 'algorithm', 'hyperparameters'],\n    num_rows: 727\n})"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"train_valid_split = dataset.train_test_split(test_size=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:18:56.757437Z","iopub.execute_input":"2024-11-17T18:18:56.757841Z","iopub.status.idle":"2024-11-17T18:18:56.776238Z","shell.execute_reply.started":"2024-11-17T18:18:56.757798Z","shell.execute_reply":"2024-11-17T18:18:56.775386Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"train_dataset = train_valid_split['train']\nvalid_dataset = train_valid_split['test']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:18:56.777491Z","iopub.execute_input":"2024-11-17T18:18:56.777873Z","iopub.status.idle":"2024-11-17T18:18:56.782151Z","shell.execute_reply.started":"2024-11-17T18:18:56.777829Z","shell.execute_reply":"2024-11-17T18:18:56.781231Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"EOS_TOKEN = tokenizer.eos_token\n\ntrain_prompt = \"\"\"Below is a description for a time series data. Write a response that gives the name of the best fitting machine learning algorithm in one word without explanation.\nThe best algorithm name should be one of this search space algorithms: AdaboostRegressor, ElasticNetRegressor,  ExtraTreesRegressor,  LassoRegressor,  LightgbmRegressor, SVR, GaussianProcessRegressor, RandomForestRegressor or  XGBoostRegressor.\n\n### DESCRIPTION:\n{}\n\n### RESPONSE:\n{}\"\"\"\n\n\n\ndef formatting_prompts_func(examples):\n    inputs       = examples[\"series_description\"]\n    outputs      = examples[\"algorithm\"]\n    texts = []\n    for input, output in zip( inputs, outputs):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = train_prompt.format( input, output) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\n\n\ndef formatting_test_prompts_func(examples):\n    inputs = examples[\"series_description\"]\n    texts = []\n\n    for input in inputs:\n        text = test_prompt.format(input) \n        texts.append(text)\n\n    return { \"text\": texts }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:18:56.783378Z","iopub.execute_input":"2024-11-17T18:18:56.783696Z","iopub.status.idle":"2024-11-17T18:18:56.794874Z","shell.execute_reply.started":"2024-11-17T18:18:56.783653Z","shell.execute_reply":"2024-11-17T18:18:56.794028Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"train_dataset = train_dataset.map(formatting_prompts_func, batched = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:18:56.795959Z","iopub.execute_input":"2024-11-17T18:18:56.796268Z","iopub.status.idle":"2024-11-17T18:18:56.865004Z","shell.execute_reply.started":"2024-11-17T18:18:56.796236Z","shell.execute_reply":"2024-11-17T18:18:56.864070Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/654 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88faf71e842b4e278ea032c1c6f21ee3"}},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"train_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:18:56.866306Z","iopub.execute_input":"2024-11-17T18:18:56.866632Z","iopub.status.idle":"2024-11-17T18:18:56.872953Z","shell.execute_reply.started":"2024-11-17T18:18:56.866584Z","shell.execute_reply":"2024-11-17T18:18:56.872069Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['dataset_name', 'series_description', 'algorithm', 'hyperparameters', 'text'],\n    num_rows: 654\n})"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"valid_dataset = valid_dataset.map(formatting_prompts_func, batched = True)\nvalid_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:18:56.873997Z","iopub.execute_input":"2024-11-17T18:18:56.874269Z","iopub.status.idle":"2024-11-17T18:18:56.914062Z","shell.execute_reply.started":"2024-11-17T18:18:56.874239Z","shell.execute_reply":"2024-11-17T18:18:56.913187Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/73 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cc86230b8ae404da3ebe80befce5fff"}},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['dataset_name', 'series_description', 'algorithm', 'hyperparameters', 'text'],\n    num_rows: 73\n})"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntraining_arguments= TrainingArguments(\n        num_train_epochs=1,\n        per_device_train_batch_size = 4,\n        gradient_accumulation_steps = 32//4,\n        gradient_checkpointing=True,\n        warmup_steps = 5,\n        max_steps = -1, # Set num_train_epochs = 1 for full training runs\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"./mistral_outputs\",\n        evaluation_strategy=\"steps\", #epoch\n        save_strategy=\"epoch\",\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:18:56.915247Z","iopub.execute_input":"2024-11-17T18:18:56.915580Z","iopub.status.idle":"2024-11-17T18:18:56.955287Z","shell.execute_reply.started":"2024-11-17T18:18:56.915547Z","shell.execute_reply":"2024-11-17T18:18:56.954374Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"from trl import  DataCollatorForCompletionOnlyLM\n\ninstruction_template=\"DESCRIPTION:\"\nresponse_template = \"RESPONSE:\"\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = training_arguments,\n    # data_collator =  DataCollatorForCompletionOnlyLM(instruction_template=instruction_template,\n    #                                                  response_template=response_template,\n    #                                                  tokenizer=tokenizer,mlm=False),\n\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:18:56.956459Z","iopub.execute_input":"2024-11-17T18:18:56.956784Z","iopub.status.idle":"2024-11-17T18:19:01.638049Z","shell.execute_reply.started":"2024-11-17T18:18:56.956751Z","shell.execute_reply":"2024-11-17T18:19:01.637167Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/654 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ef09859aa0842f485941ca07fe08dc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/73 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"425952eaa92041d28ba448a0d8b02e15"}},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:19:01.639382Z","iopub.execute_input":"2024-11-17T18:19:01.639725Z","iopub.status.idle":"2024-11-17T18:19:01.646326Z","shell.execute_reply.started":"2024-11-17T18:19:01.639689Z","shell.execute_reply":"2024-11-17T18:19:01.645318Z"}},"outputs":[{"name":"stdout","text":"GPU = Tesla T4. Max memory = 14.741 GB.\n5.145 GB of memory reserved.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import time\nstart= time.time()\ntrainer_stats = trainer.train()\nprint((time.time()-start)/60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:19:01.647477Z","iopub.execute_input":"2024-11-17T18:19:01.648285Z","iopub.status.idle":"2024-11-17T18:37:10.859806Z","shell.execute_reply.started":"2024-11-17T18:19:01.648234Z","shell.execute_reply":"2024-11-17T18:37:10.858678Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 654 | Num Epochs = 1\nO^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 8\n\\        /    Total batch size = 32 | Total steps = 20\n \"-____-\"     Number of trainable parameters = 491,257,856\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  Â·Â·Â·Â·Â·Â·Â·Â·\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113604322220756, max=1.0â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8b6da28daee43a48a28951701c2c9b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241117_181919-7qdsvyc2</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/hassangamal189-giza-systems/huggingface/runs/7qdsvyc2' target=\"_blank\">./mistral_outputs</a></strong> to <a href='https://wandb.ai/hassangamal189-giza-systems/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/hassangamal189-giza-systems/huggingface' target=\"_blank\">https://wandb.ai/hassangamal189-giza-systems/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/hassangamal189-giza-systems/huggingface/runs/7qdsvyc2' target=\"_blank\">https://wandb.ai/hassangamal189-giza-systems/huggingface/runs/7qdsvyc2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [20/20 17:18, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.467100</td>\n      <td>2.461712</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.448900</td>\n      <td>2.368804</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.372100</td>\n      <td>2.167445</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.148600</td>\n      <td>1.898919</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.895000</td>\n      <td>1.612520</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.595900</td>\n      <td>1.327621</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.331200</td>\n      <td>1.102626</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.067200</td>\n      <td>0.917982</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.891900</td>\n      <td>0.788918</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.781600</td>\n      <td>0.674701</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.678300</td>\n      <td>0.592369</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.597400</td>\n      <td>0.529429</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.522000</td>\n      <td>0.486668</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.458400</td>\n      <td>0.449844</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.475700</td>\n      <td>0.423552</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.454400</td>\n      <td>0.403106</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.403400</td>\n      <td>0.387453</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.377500</td>\n      <td>0.376333</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.377100</td>\n      <td>0.369244</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.379800</td>\n      <td>0.365583</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"18.153227162361144\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"#@title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)# save tuned model\n\n#To save the final model as LoRA adapters\nused_percentage = round(used_memory         /max_memory*100, 3)\nlora_percentage = round(used_memory_for_lora/max_memory*100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:37:10.861300Z","iopub.execute_input":"2024-11-17T18:37:10.861617Z","iopub.status.idle":"2024-11-17T18:37:10.872085Z","shell.execute_reply.started":"2024-11-17T18:37:10.861583Z","shell.execute_reply":"2024-11-17T18:37:10.871316Z"}},"outputs":[{"name":"stdout","text":"1086.3193 seconds used for training.\n18.11 minutes used for training.\nPeak reserved memory = 12.436 GB.\nPeak reserved memory for training = 7.291 GB.\nPeak reserved memory % of max memory = 84.363 %.\nPeak reserved memory for training % of max memory = 49.461 %.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"test_prompt = \"\"\"Below is a description for a time series data. Write a response that gives the name of the best fitting machine learning algorithm in one word without explanation.\n\nThe best algorithm name should be one of this search space algorithms: AdaboostClassifier, ElasticNetClassifier,  LassoClassifier,  LightgbmClassifier, SVC, GaussianProcessClassifier, RandomForestClassifier or  XGBoostClassifier.\n\n\n\n### DESCRIPTION:\n\n{}\n\n\n\n### RESPONSE:\n\n\"\"\"\n\n\nvalid_dataset = test_datasetalid_dataset.map(formatting_test_prompts_func, batched = True)\n\nvalid_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:38:52.011931Z","iopub.execute_input":"2024-11-17T18:38:52.012946Z","iopub.status.idle":"2024-11-17T18:38:52.049459Z","shell.execute_reply.started":"2024-11-17T18:38:52.012902Z","shell.execute_reply":"2024-11-17T18:38:52.048603Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/73 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c189f0e52c7b47778eb1499a120e9287"}},"metadata":{}},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['dataset_name', 'series_description', 'algorithm', 'hyperparameters', 'text'],\n    num_rows: 73\n})"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"valid_dataset['text'][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:39:34.960611Z","iopub.execute_input":"2024-11-17T18:39:34.961538Z","iopub.status.idle":"2024-11-17T18:39:34.969523Z","shell.execute_reply.started":"2024-11-17T18:39:34.961498Z","shell.execute_reply":"2024-11-17T18:39:34.968533Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"'Below is a description for a time series data. Write a response that gives the name of the best fitting machine learning algorithm in one word without explanation.\\n\\nThe best algorithm name should be one of this search space algorithms: AdaboostClassifier, ElasticNetClassifier,  LassoClassifier,  LightgbmClassifier, SVC, GaussianProcessClassifier, RandomForestClassifier or  XGBoostClassifier.\\n\\n\\n\\n### DESCRIPTION:\\n\\nA multivariate classification time-series dataset consists of 7121 samples and 16 features with 16 numerical and 0 categorical features. Each instance has a window length of 24. The dataset has a sampling rate of 60.0 minutes. The dataset has a missing values percentage of 0.0%. The missing values percentages for numerical features range from 0 to 0 with mean 0.00 and standard deviation 0.00.\\n The target column has 3 classes with entropy value 1.37 showing a Unbalanced dataset. Among the 7121 samples the target ground-truth class has changed 1162 times representing a percentage of 16.40%. There are 16 features in the dataset\\n Among the numerical predictors, the series has 16 numerical features detected as Stationary out of the 16 numerical features using the dickey-fuller test and the rest are Unstationary. 15 of them are Multiplicative time-series features and the rest are Additive time-series features. There is an average of 0 seasonality components detected in the numerical predictors. The top 0 common seasonality components are represented using sinusoidal waves. The numerical predictors also exhibit skewness values ranging from 0.017. to 22.516 and kurtosis values of 0.77 to 605.91. The fractal dimension analysis yields values ranging from -0.66 to -0.13 indicating a Complex and Irregular time-series structure for the numerical predictors. The correlation values among the numerical predictors have a minimum of -0.95, maximum 1.00, mean 0.11, and standard deviation 0.54. The count of numerical predictors with outliers is 16 with the minimum percentage of 1.88%, maximum percentage of 47.45%, average percentage of 40.79%, and standard deviation percentage of 13.52%.\\n\\nThe dataset is converted into a simple classification task by extracting the previously described features.\\n\\n\\n\\n### RESPONSE:\\n\\n'"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"valid_dataset['algorithm'][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:39:53.899982Z","iopub.execute_input":"2024-11-17T18:39:53.900780Z","iopub.status.idle":"2024-11-17T18:39:53.908142Z","shell.execute_reply.started":"2024-11-17T18:39:53.900737Z","shell.execute_reply":"2024-11-17T18:39:53.907141Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"'AdaboostClassifier'"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\ninputs = tokenizer(\n\n[valid_dataset['text'][0]], return_tensors = \"pt\").to(\"cuda\")\n\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n\ntokenizer.batch_decode(outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:40:07.993081Z","iopub.execute_input":"2024-11-17T18:40:07.994025Z","iopub.status.idle":"2024-11-17T18:40:08.838999Z","shell.execute_reply.started":"2024-11-17T18:40:07.993982Z","shell.execute_reply":"2024-11-17T18:40:08.837871Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"['<|begin_of_text|>Below is a description for a time series data. Write a response that gives the name of the best fitting machine learning algorithm in one word without explanation.\\n\\nThe best algorithm name should be one of this search space algorithms: AdaboostClassifier, ElasticNetClassifier,  LassoClassifier,  LightgbmClassifier, SVC, GaussianProcessClassifier, RandomForestClassifier or  XGBoostClassifier.\\n\\n\\n\\n### DESCRIPTION:\\n\\nA multivariate classification time-series dataset consists of 7121 samples and 16 features with 16 numerical and 0 categorical features. Each instance has a window length of 24. The dataset has a sampling rate of 60.0 minutes. The dataset has a missing values percentage of 0.0%. The missing values percentages for numerical features range from 0 to 0 with mean 0.00 and standard deviation 0.00.\\n The target column has 3 classes with entropy value 1.37 showing a Unbalanced dataset. Among the 7121 samples the target ground-truth class has changed 1162 times representing a percentage of 16.40%. There are 16 features in the dataset\\n Among the numerical predictors, the series has 16 numerical features detected as Stationary out of the 16 numerical features using the dickey-fuller test and the rest are Unstationary. 15 of them are Multiplicative time-series features and the rest are Additive time-series features. There is an average of 0 seasonality components detected in the numerical predictors. The top 0 common seasonality components are represented using sinusoidal waves. The numerical predictors also exhibit skewness values ranging from 0.017. to 22.516 and kurtosis values of 0.77 to 605.91. The fractal dimension analysis yields values ranging from -0.66 to -0.13 indicating a Complex and Irregular time-series structure for the numerical predictors. The correlation values among the numerical predictors have a minimum of -0.95, maximum 1.00, mean 0.11, and standard deviation 0.54. The count of numerical predictors with outliers is 16 with the minimum percentage of 1.88%, maximum percentage of 47.45%, average percentage of 40.79%, and standard deviation percentage of 13.52%.\\n\\nThe dataset is converted into a simple classification task by extracting the previously described features.\\n\\n\\n\\n### RESPONSE:\\n\\nAdaboostClassifier\\n<|end_of_text|>']"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"test_responses=[]\n\n# get all test data inference result\n\nfor test_prompt in valid_dataset['text']:\n\n  inputs= tokenizer(\n\n  [test_prompt], return_tensors = \"pt\").to(\"cuda\")\n\n\n\n  outputs = model.generate(**inputs, max_new_tokens = 10, use_cache = True)\n\n  test_responses.append(tokenizer.batch_decode(outputs))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:40:39.433473Z","iopub.execute_input":"2024-11-17T18:40:39.434370Z","iopub.status.idle":"2024-11-17T18:41:34.211770Z","shell.execute_reply.started":"2024-11-17T18:40:39.434326Z","shell.execute_reply":"2024-11-17T18:41:34.210953Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"df = valid_dataset.to_pandas()\ndf['model_responses']= test_responses\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:42:30.584273Z","iopub.execute_input":"2024-11-17T18:42:30.584664Z","iopub.status.idle":"2024-11-17T18:42:30.612189Z","shell.execute_reply.started":"2024-11-17T18:42:30.584616Z","shell.execute_reply":"2024-11-17T18:42:30.611318Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"                       dataset_name  \\\n0   1031-8-2-1-2-classification.csv   \n1  1031-40-2-1-5-classification.csv   \n2       1030-503-classification.csv   \n3        1028-32-classification.csv   \n4  1031-21-2-1-3-classification.csv   \n\n                                  series_description             algorithm  \\\n0  A multivariate classification time-series data...    AdaboostClassifier   \n1  A multivariate classification time-series data...     XGBoostClassifier   \n2  A multivariate classification time-series data...  ElasticNetClassifier   \n3  A multivariate classification time-series data...     XGBoostClassifier   \n4  A multivariate classification time-series data...    AdaboostClassifier   \n\n                                     hyperparameters  \\\n0  {'estimator': DecisionTreeClassifier(max_depth...   \n1  {'learning_rate': 0.1, 'max_depth': 5, 'n_esti...   \n2  {'C': 1000.0, 'l1_ratio': 0.0001, 'penalty': '...   \n3  {'learning_rate': 0.1, 'max_depth': 5, 'n_esti...   \n4  {'estimator': DecisionTreeClassifier(max_depth...   \n\n                                                text  \\\n0  Below is a description for a time series data....   \n1  Below is a description for a time series data....   \n2  Below is a description for a time series data....   \n3  Below is a description for a time series data....   \n4  Below is a description for a time series data....   \n\n                                     model_responses  \n0  [<|begin_of_text|>Below is a description for a...  \n1  [<|begin_of_text|>Below is a description for a...  \n2  [<|begin_of_text|>Below is a description for a...  \n3  [<|begin_of_text|>Below is a description for a...  \n4  [<|begin_of_text|>Below is a description for a...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dataset_name</th>\n      <th>series_description</th>\n      <th>algorithm</th>\n      <th>hyperparameters</th>\n      <th>text</th>\n      <th>model_responses</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1031-8-2-1-2-classification.csv</td>\n      <td>A multivariate classification time-series data...</td>\n      <td>AdaboostClassifier</td>\n      <td>{'estimator': DecisionTreeClassifier(max_depth...</td>\n      <td>Below is a description for a time series data....</td>\n      <td>[&lt;|begin_of_text|&gt;Below is a description for a...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1031-40-2-1-5-classification.csv</td>\n      <td>A multivariate classification time-series data...</td>\n      <td>XGBoostClassifier</td>\n      <td>{'learning_rate': 0.1, 'max_depth': 5, 'n_esti...</td>\n      <td>Below is a description for a time series data....</td>\n      <td>[&lt;|begin_of_text|&gt;Below is a description for a...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1030-503-classification.csv</td>\n      <td>A multivariate classification time-series data...</td>\n      <td>ElasticNetClassifier</td>\n      <td>{'C': 1000.0, 'l1_ratio': 0.0001, 'penalty': '...</td>\n      <td>Below is a description for a time series data....</td>\n      <td>[&lt;|begin_of_text|&gt;Below is a description for a...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1028-32-classification.csv</td>\n      <td>A multivariate classification time-series data...</td>\n      <td>XGBoostClassifier</td>\n      <td>{'learning_rate': 0.1, 'max_depth': 5, 'n_esti...</td>\n      <td>Below is a description for a time series data....</td>\n      <td>[&lt;|begin_of_text|&gt;Below is a description for a...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1031-21-2-1-3-classification.csv</td>\n      <td>A multivariate classification time-series data...</td>\n      <td>AdaboostClassifier</td>\n      <td>{'estimator': DecisionTreeClassifier(max_depth...</td>\n      <td>Below is a description for a time series data....</td>\n      <td>[&lt;|begin_of_text|&gt;Below is a description for a...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"from difflib import get_close_matches\n\n# List of valid algorithm names\nvalid_algorithms = [\n    'GaussianProcessClassifier',\n    'ElasticNetClassifier',\n    'LassoClassifier',\n    'AdaboostClassifier',\n    'XGBoostClassifier',\n    'RandomForestClassifier',\n    'SVC',\n    'LightgbmClassifier'\n]\n\n# Ensure you are comparing the predicted algorithm names with the actual algorithm names\npredictions = []\n\nfor response in test_responses:\n    try:\n        # Extracting the predicted algorithm from the response text\n        if '### RESPONSE:' in response[0]:\n            # Split at \"### RESPONSE:\" to isolate the relevant part\n            response_text = response[0].split('### RESPONSE:')[1]\n            # Remove unwanted tokens like '</s>' and extra newlines\n            response_text = response_text.replace('</s>', '').strip()\n            # Extract the first valid word (algorithm name)\n            predicted_algo = response_text.split()[0]\n\n            # Validate and correct the predicted algorithm name\n            predicted_algo = get_close_matches(predicted_algo, valid_algorithms, n=1, cutoff=0.5)\n            predicted_algo = predicted_algo[0] if predicted_algo else \"\"  # Take closest match or empty\n        else:\n            predicted_algo = \"\"  # Handle cases where \"### RESPONSE:\" is missing\n\n        predictions.append(predicted_algo)\n    except Exception as e:\n        print(f\"Error parsing response: {response}, Error: {e}\")\n        predictions.append(\"\")  # Append an empty string for invalid responses\n\n# Convert actual_data to a list if it's a DataFrame column\nactual_data = df['algorithm'].tolist()\n\n# Debugging Outputs\noutputs = model.generate(**inputs, max_new_tokens=10, use_cache=True)\ndecoded_output = tokenizer.batch_decode(outputs)\nprint(\"\\nDecoded Output:\", decoded_output)  # Debug decoded output\n\n# Compute the accuracy by comparing the predictions to the actual algorithm names\naccuracy = sum(1 for true, pred in zip(actual_data, predictions) if true == pred) / len(actual_data)\nprint(\"Accuracy:\", accuracy)\n\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\nrecall = recall_score(actual_data, predictions, average='weighted')\nf1 = f1_score(actual_data, predictions, average='weighted')\n\nprint(\"Recall:\", recall)\nprint(\"F1 Score:\", f1)\n\n# Debug the first few predictions and actual values\nprint(\"\\nActual Data (First 5):\", actual_data[:5])\nprint(\"Predictions (First 5):\", predictions[:5])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:42:53.630533Z","iopub.execute_input":"2024-11-17T18:42:53.631523Z","iopub.status.idle":"2024-11-17T18:42:54.525030Z","shell.execute_reply.started":"2024-11-17T18:42:53.631479Z","shell.execute_reply":"2024-11-17T18:42:54.524022Z"}},"outputs":[{"name":"stdout","text":"\nDecoded Output: ['<|begin_of_text|>Below is a description for a time series data. Write a response that gives the name of the best fitting machine learning algorithm in one word without explanation.\\n\\nThe best algorithm name should be one of this search space algorithms: AdaboostClassifier, ElasticNetClassifier,  LassoClassifier,  LightgbmClassifier, SVC, GaussianProcessClassifier, RandomForestClassifier or  XGBoostClassifier.\\n\\n\\n\\n### DESCRIPTION:\\n\\nA multivariate classification time-series dataset consists of 7477 samples and 15 features with 7 numerical and 8 categorical features. Each instance has a window length of 24. The dataset has a sampling rate of 60.0 minutes. The dataset has a missing values percentage of 0.0%. The missing values percentages for numerical features range from 0 to 0 with mean 0.00 and standard deviation 0.00. Similarly, the missing values percentages for categorical features range from 0 to 0 with mean 0.0 and standard deviation 0.0.\\n The target column has 3 classes with entropy value 1.46 showing a Unbalanced dataset. Among the 7477 samples the target ground-truth class has changed 1029 times representing a percentage of 13.83%. There are 15 features in the dataset with a ratio of numerical to categorical features of 0.875.\\n Among the numerical predictors, the series has 7 numerical features detected as Stationary out of the 7 numerical features using the dickey-fuller test and the rest are Unstationary. 7 of them are Multiplicative time-series features and the rest are Additive time-series features. There is an average of 0 seasonality components detected in the numerical predictors. The top 0 common seasonality components are represented using sinusoidal waves. The numerical predictors also exhibit skewness values ranging from 0.066. to 3.096 and kurtosis values of 0.20 to 10.86. The fractal dimension analysis yields values ranging from -0.66 to -0.13 indicating a Complex and Irregular time-series structure for the numerical predictors. The correlation values among the numerical predictors have a minimum of -0.45, maximum 1.00, mean 0.29, and standard deviation 0.53. The count of numerical predictors with outliers is 7 with the minimum percentage of 0.11%, maximum percentage of 16.20%, average percentage of 2.61%, and standard deviation percentage of 6.00%.\\n Among the categorical predictors, the count of symbols ranges from 41 to 73 with a minimum entropy value 0.9917708855272739, maximum entropy 5.76740921961984, mean 4.546935167528349, and standard deviation 1.3789044013708147,\\nThe dataset is converted into a simple classification task by extracting the previously described features.\\n\\n\\n\\n### RESPONSE:\\n\\nAdaboostClassifier\\n<|end_of_text|>']\nAccuracy: 0.3150684931506849\nRecall: 0.3150684931506849\nF1 Score: 0.1509703196347032\n\nActual Data (First 5): ['AdaboostClassifier', 'XGBoostClassifier', 'ElasticNetClassifier', 'XGBoostClassifier', 'AdaboostClassifier']\nPredictions (First 5): ['AdaboostClassifier', 'AdaboostClassifier', 'AdaboostClassifier', 'AdaboostClassifier', 'AdaboostClassifier']\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"# save tuned model\n\n","metadata":{}},{"cell_type":"code","source":"# %%capture\n# !pip install transformers huggingface_hub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:37:10.873215Z","iopub.execute_input":"2024-11-17T18:37:10.873481Z","iopub.status.idle":"2024-11-17T18:37:14.099604Z","shell.execute_reply.started":"2024-11-17T18:37:10.873452Z","shell.execute_reply":"2024-11-17T18:37:14.098398Z"}},"outputs":[{"name":"stderr","text":"[2024-11-17 18:37:13 +0000] [188] [INFO] Handling signal: int\n[2024-11-17 18:37:13 +0000] [191] [INFO] Worker exiting (pid: 191)\n[2024-11-17 18:37:13 +0000] [192] [INFO] Worker exiting (pid: 192)\n[2024-11-17 18:37:13 +0000] [190] [INFO] Worker exiting (pid: 190)\n[2024-11-17 18:37:13 +0000] [193] [INFO] Worker exiting (pid: 193)\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# from huggingface_hub import login\n\n# login(token=\"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\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:37:14.101123Z","iopub.execute_input":"2024-11-17T18:37:14.101550Z","iopub.status.idle":"2024-11-17T18:37:14.206192Z","shell.execute_reply.started":"2024-11-17T18:37:14.101492Z","shell.execute_reply":"2024-11-17T18:37:14.205171Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# # Online saving on HF\n\n# new_model_adabtor= \"unsloth-Llama-tuned\"\n\n# model.push_to_hub(new_model_adabtor)\n\n# tokenizer.push_to_hub(new_model_adabtor)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}